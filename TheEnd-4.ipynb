{"cells": [{"cell_type": "code", "execution_count": 82, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "tDfzk2mQtGBR", "outputId": "7995b553-47c3-4cc6-9c81-91a61d89c890"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Requirement already satisfied: pandasql in /opt/conda/miniconda3/lib/python3.8/site-packages (0.7.3)\nRequirement already satisfied: sqlalchemy in /opt/conda/miniconda3/lib/python3.8/site-packages (from pandasql) (1.4.36)\nRequirement already satisfied: pandas in /opt/conda/miniconda3/lib/python3.8/site-packages (from pandasql) (1.2.5)\nRequirement already satisfied: numpy in /opt/conda/miniconda3/lib/python3.8/site-packages (from pandasql) (1.19.5)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/miniconda3/lib/python3.8/site-packages (from pandas->pandasql) (2.8.0)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/miniconda3/lib/python3.8/site-packages (from pandas->pandasql) (2022.1)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/miniconda3/lib/python3.8/site-packages (from sqlalchemy->pandasql) (1.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/miniconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->pandasql) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 1764:===================================================>(199 + 1) / 200]\r"}], "source": "import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import roc_curve, roc_auc_score, classification_report, accuracy_score, confusion_matrix \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime, timedelta\nimport re\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, f1_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.decomposition import PCA\nimport os\n!pip install pandasql\nimport pandasql as ps\n!pip install pyspark -q\nimport pyspark\nfrom pyspark.sql import SparkSession, SQLContext, Window\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, DoubleType, BooleanType\nfrom pyspark.sql.functions import col,isnan,when,count"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "gs://mf810finalproject-bucket/archive-3/\n"}], "source": "bucket = spark._jsc.hadoopConfiguration().get(\"fs.gs.system.bucket\")\ndata = \"gs://\" + bucket + \"/archive-3/\"\nprint(data)"}, {"cell_type": "code", "execution_count": 29, "metadata": {}, "outputs": [{"ename": "FileNotFoundError", "evalue": "[Errno 2] No such file or directory: './mf810finalproject-bucket/archive-3/'", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)", "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m storage\n\u001b[0;32m----> 2\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mClient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_service_account_json\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./mf810finalproject-bucket/archive-3/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m bucket \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mbucket(bucketname)\n\u001b[1;32m      4\u001b[0m blobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(bucket\u001b[38;5;241m.\u001b[39mlist_blobs(prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malfabattle2_train_transactions_contest/\u001b[39m\u001b[38;5;124m'\u001b[39m))\n", "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.8/site-packages/google/cloud/client/__init__.py:106\u001b[0m, in \u001b[0;36m_ClientFactoryMixin.from_service_account_json\u001b[0;34m(cls, json_credentials_path, *args, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_service_account_json\u001b[39m(\u001b[38;5;28mcls\u001b[39m, json_credentials_path, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;124;03m\"\"\"Factory to retrieve JSON credentials while creating client.\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m    :type json_credentials_path: str\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m             and the credentials created by the factory.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_credentials_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m json_fi:\n\u001b[1;32m    107\u001b[0m         credentials_info \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(json_fi)\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_service_account_info(credentials_info, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n", "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './mf810finalproject-bucket/archive-3/'"]}], "source": "from google.cloud import storage\nclient = storage.Client.from_service_account_json('./mf810finalproject-bucket/archive-3/')\nbucket = client.bucket(bucketname)\nblobs = list(bucket.list_blobs(prefix='alfabattle2_train_transactions_contest/'))\nblobs"}, {"cell_type": "code", "execution_count": 42, "metadata": {"id": "zTicoBeLtOS1"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "train_target = spark.read.option(\"inferSchema\",\"true\").csv(data + \"alfabattle2_train_target.csv\")\ntest_target = spark.read.option(\"inferSchema\",\"true\").csv(data + \"alfabattle2_test_target_contest.csv\")\nsample = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(data + \"alfabattle2_alpha_sample.csv\")"}, {"cell_type": "code", "execution_count": 59, "metadata": {}, "outputs": [], "source": "train_target1 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_000_0_to_23646.parquet\")\ntrain_target2 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_001_23647_to_47415.parquet\")\ntrain_target3 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_002_47416_to_70092.parquet\")\ntrain_target4 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_003_70093_to_92989.parquet\")\ntrain_target5 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_004_92990_to_115175.parquet\")\ntrain_target6 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_005_115176_to_138067.parquet\")\ntrain_target7 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_006_138068_to_159724.parquet\")\ntrain_target8 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_007_159725_to_180735.parquet\")\ntrain_target9 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_008_180736_to_202834.parquet\")\ntrain_target10 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_009_202835_to_224283.parquet\")"}, {"cell_type": "code", "execution_count": 60, "metadata": {}, "outputs": [], "source": "train_target11 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_010_224284_to_245233.parquet\")\ntrain_target12 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_011_245234_to_265281.parquet\")\ntrain_target13 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_012_265282_to_285632.parquet\")\ntrain_target14 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_013_285633_to_306877.parquet\")\ntrain_target15 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_014_306878_to_329680.parquet\")\ntrain_target16 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_015_329681_to_350977.parquet\")\ntrain_target17 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_016_350978_to_372076.parquet\")\ntrain_target18 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_017_372077_to_392692.parquet\")\ntrain_target19= spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_018_392693_to_413981.parquet\")\ntrain_target20 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_019_413982_to_434478.parquet\")"}, {"cell_type": "code", "execution_count": 63, "metadata": {}, "outputs": [], "source": "train_target21 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_020_434479_to_455958.parquet\")\ntrain_target22 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_021_455959_to_477221.parquet\")\ntrain_target23 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_022_477222_to_496751.parquet\")\ntrain_target24 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_023_496752_to_517332.parquet\")\ntrain_target25 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_024_517333_to_537036.parquet\")\ntrain_target26 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_025_537037_to_557423.parquet\")\ntrain_target27 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_026_557424_to_576136.parquet\")\ntrain_target28 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_027_576137_to_595745.parquet\")\ntrain_target29 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_028_595746_to_615602.parquet\")\ntrain_target30 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_029_615603_to_635004.parquet\")"}, {"cell_type": "code", "execution_count": 65, "metadata": {}, "outputs": [], "source": "train_target31 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_030_635005_to_654605.parquet\")\ntrain_target32 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_031_654606_to_673656.parquet\")\ntrain_target33 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_032_673657_to_696025.parquet\")\ntrain_target34 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_033_696026_to_714545.parquet\")\ntrain_target35 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_034_714546_to_733168.parquet\")\ntrain_target36 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_035_733169_to_752514.parquet\")\ntrain_target37 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_036_752515_to_770940.parquet\")\ntrain_target38 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_037_770941_to_788380.parquet\")\ntrain_target39 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_038_788381_to_805771.parquet\")\ntrain_target40 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_039_805772_to_823299.parquet\")"}, {"cell_type": "code", "execution_count": 67, "metadata": {}, "outputs": [], "source": "train_target41 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_040_823300_to_841218.parquet\")\ntrain_target42 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_041_841219_to_859270.parquet\")\ntrain_target43 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_042_859271_to_878521.parquet\")\ntrain_target44 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_043_878522_to_896669.parquet\")\ntrain_target45 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_044_896670_to_916056.parquet\")\ntrain_target46 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_045_916057_to_935131.parquet\")\ntrain_target47 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_046_935132_to_951695.parquet\")\ntrain_target48 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_047_951696_to_970383.parquet\")\ntrain_target49 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_048_970384_to_987313.parquet\")\ntrain_target50 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_train_transactions_contest/train_transactions_contest/part_049_987314_to_1003050.parquet\")"}, {"cell_type": "code", "execution_count": 54, "metadata": {}, "outputs": [], "source": "test_target1 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_000_1063620_to_1074462.parquet\")\ntest_target2 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data +\"alfabattle2_test_transactions_contest/test_transactions_contest/part_001_1074463_to_1085303.parquet\")\ntest_target3 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_002_1085304_to_1095174.parquet\")\ntest_target4 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_003_1095175_to_1105002.parquet\")\ntest_target5 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_004_1105003_to_1116054.parquet\")\ntest_target6 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_005_1116055_to_1127527.parquet\")\ntest_target7 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_006_1127528_to_1137672.parquet\")\ntest_target8 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_007_1137673_to_1147504.parquet\")\ntest_target9 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_008_1147505_to_1157749.parquet\")\ntest_target10 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_009_1157750_to_1167980.parquet\")"}, {"cell_type": "code", "execution_count": 55, "metadata": {}, "outputs": [], "source": "test_target11 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_010_1167981_to_1178851.parquet\")\ntest_target12 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_011_1178852_to_1190630.parquet\")\ntest_target13 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_012_1190631_to_1200939.parquet\")\ntest_target14 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_013_1200940_to_1211425.parquet\")\ntest_target15 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_014_1211426_to_1222122.parquet\")\ntest_target16 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_015_1222123_to_1232298.parquet\")\ntest_target17 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_016_1232299_to_1242388.parquet\")\ntest_target18 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_017_1242389_to_1252416.parquet\") \ntest_target19 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_018_1252417_to_1262614.parquet\")\ntest_target20 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_019_1262615_to_1273376.parquet\")"}, {"cell_type": "code", "execution_count": 56, "metadata": {}, "outputs": [], "source": "test_target21 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_020_1273377_to_1283831.parquet\")\ntest_target22 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_021_1283832_to_1294494.parquet\")\ntest_target23 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_022_1294495_to_1304964.parquet\")\ntest_target24 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_023_1304965_to_1314698.parquet\")\ntest_target25 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_024_1314699_to_1324518.parquet\")\ntest_target26 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_025_1324519_to_1334901.parquet\")\ntest_target27 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_026_1334902_to_1345587.parquet\")\ntest_target28 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_027_1345588_to_1355874.parquet\")\ntest_target29 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_028_1355875_to_1366314.parquet\")\ntest_target30 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_029_1366315_to_1376991.parquet\")"}, {"cell_type": "code", "execution_count": 57, "metadata": {}, "outputs": [], "source": "test_target31 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_030_1376992_to_1386419.parquet\")\nTest_target32 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_031_1386420_to_1395884.parquet\")\ntest_target33 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_032_1395885_to_1405390.parquet\")\ntest_target34 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_033_1405391_to_1416489.parquet\")\ntest_target35 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_034_1416492_to_1426763.parquet\")\ntest_target36 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_035_1426764_to_1436400.parquet\")\ntest_target37 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_036_1436401_to_1448080.parquet\")\ntest_target38 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_037_1448081_to_1459730.parquet\")\ntest_target39 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_038_1459731_to_1470134.parquet\")\ntest_target40 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_039_1470135_to_1479802.parquet\")"}, {"cell_type": "code", "execution_count": 58, "metadata": {}, "outputs": [], "source": "test_target41 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_040_1479803_to_1489232.parquet\")\ntest_target42 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_041_1489233_to_1499712.parquet\")\ntest_target43 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_042_1499713_to_1510447.parquet\")\ntest_target44 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_043_1510448_to_1520793.parquet\")\ntest_target45 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_044_1520794_to_1531282.parquet\")\ntest_target46 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_045_1531283_to_1541445.parquet\")\ntest_target47 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_046_1541446_to_1551040.parquet\")\ntest_target48 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_047_1551041_to_1560328.parquet\")\ntest_target49 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_048_1560329_to_1570341.parquet\")\ntest_target50 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").parquet(data + \"alfabattle2_test_transactions_contest/test_transactions_contest/part_049_1570342_to_1580442.parquet\")"}, {"cell_type": "code", "execution_count": 68, "metadata": {}, "outputs": [], "source": "train = train_target1.union(train_target2).union(train_target3).union(train_target4).union(train_target5).union(train_target6).union(train_target7).union(train_target8).union(train_target9).union(train_target10)\ntrain = train.union(train_target11).union(train_target12).union(train_target13).union(train_target14).union(train_target15).union(train_target16).union(train_target17).union(train_target18).union(train_target19).union(train_target20)\ntrain = train.union(train_target21).union(train_target22).union(train_target23).union(train_target24).union(train_target25).union(train_target26).union(train_target27).union(train_target28).union(train_target29).union(train_target30)\ntrain = train.union(train_target31).union(train_target32).union(train_target33).union(train_target34).union(train_target35).union(train_target36).union(train_target37).union(train_target38).union(train_target39).union(train_target40)\ntrain = train.union(train_target41).union(train_target42).union(train_target43).union(train_target44).union(train_target45).union(train_target46).union(train_target47).union(train_target48).union(train_target49).union(train_target50)\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "test = test_target1.union(test_target3).union(test_target3).union(test_target).union(test_target).union(test_target).union(test_target).union(test_target).union(test_target).union(test_target)\ntest = test.union(test_target11).union(test_target12).union(test_target13).union(test_target14).union(test_target15).union(test_target16).union(test_target17).union(test_target18).union(test_target19).union(test_target20)\ntest = test.union(test_target21).union(test_target22).union(test_target23).union(test_target24).union(test_target25).union(test_target26).union(test_target27).union(test_target28).union(test_target29).union(test_target30)\ntest = test.union(test_target31).union(test_target32).union(test_target33).union(test_target34).union(test_target35).union(test_target36).union(test_target37).union(test_target38).union(test_target39).union(test_target40)\ntest = test.union(test_target41).union(test_target42).union(test_target43).union(test_target44).union(test_target45).union(test_target46).union(test_target47).union(test_target48).union(test_target49).union(test_target50)\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "train.show(5)"}, {"cell_type": "code", "execution_count": 83, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 1765:===================================================>(199 + 1) / 200]\r"}, {"name": "stdout", "output_type": "stream", "text": "+------+----+--------+--------------+---------+--------------+--------------------+--------------+--------------+-----------+---+-------+----+------------+-----------+----+-----------+----------+---------+------------------+-----------------+\n|app_id|amnt|currency|operation_kind|card_type|operation_type|operation_type_group|ecommerce_flag|payment_system|income_flag|mcc|country|city|mcc_category|day_of_week|hour|days_before|weekofyear|hour_diff|transaction_number|__index_level_0__|\n+------+----+--------+--------------+---------+--------------+--------------------+--------------+--------------+-----------+---+-------+----+------------+-----------+----+-----------+----------+---------+------------------+-----------------+\n|     0|   0|       0|             0|        0|             0|                   0|             0|             0|          0|  0|      0|   0|           0|          0|   0|          0|         0|        0|                 0|                0|\n+------+----+--------+--------------+---------+--------------+--------------------+--------------+--------------+-----------+---+-------+----+------------+-----------+----+-----------+----------+---------+------------------+-----------------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "train.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in train.columns]).show()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#train.printSchema()\ntrain.select(\"operation_kind\").distinct().show()\n#train.describe().show()\n#train.select(\"operation_type\").distinct().show()\n#train.select(\"operation_type_group\").distinct().show()\n#train.select(\"ecommerce_flap\").distinct().show()\n#train.select(\"payment_system\").distinct().show()\n#train.select(\"income_flap\").distinct().show()\n#train.select(\"mcc\").distinct().show()\n#train.select(\"country\").distinct().show()\n#train.select(\"city\").distinct().show()\n#train.select(\"mcc_category\").distinct().show()\n#train.select(\"hour_diff\").distinct().show()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "train.select(\"app_id\").distinct().count()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "train.select(\"currency\").distinct().count()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "train.select(\"operation_kind\").distinct().count()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "train.select(\"card_type\").distinct().count()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "train.select(\"operation_type\").distinct().count()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "train.select(\"operation_type_group\").distinct().count()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "train.select(\"ecommerce_flap\").distinct().count()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "train.select(\"payment_system\").distinct().count()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "train.select(\"income_flap\").distinct().count()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "train.select(\"mcc\").distinct().count()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "train.select(\"country\").distinct().count()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "train.select(\"city\").distinct().count()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "train.select(\"mcc_category\").distinct().count()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "train.select(\"hour_diff\").distinct().count()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "test.show(5)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "train_target.show(5)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "train_target.select('_c1').distinct().show()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "test_target.show(5)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "sample.show(5)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "train.filter(train.app_id == 5).show()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "train.groupby('app_id').count().show()"}, {"cell_type": "markdown", "metadata": {}, "source": "SparkSQL to manipulate variables:"}, {"cell_type": "code", "execution_count": 70, "metadata": {"id": "eRWSFJ7OtRpK"}, "outputs": [], "source": "# left join train_target to train with similar app_id\ntrainadd = train.join(train_target,train.app_id == train_target._c0,\"left\")\n#trainadd.show()\n\n# create dataframe group by app_id, add variables, save SQL as 'new_train'\ntrainadd.createOrReplaceTempView(\"train0\")\nnew_train = spark.sql(\"SELECT app_id,max(amnt)AS maa,min(amnt)AS mia,AVG(amnt)AS ava,VARIANCE(amnt)AS vaa,avg(currency)AS currency,avg(operation_kind)AS operation_kind,avg(card_type)AS card_type,avg(operation_type)AS operation_type,avg(operation_type_group)AS operation_type_group,avg(ecommerce_flag)AS ecommerce_flag,avg(payment_system)AS payment_system,avg(income_flag)AS income_flag,avg(mcc)AS mcc,avg(mcc_category)AS mcc_category,avg(hour_diff)AS hour_diff,max(transaction_number)AS transaction_number,max(_c0)AS _c0,max(_c1)AS _c1,max(_c2)AS _c2 FROM train0 GROUP BY app_id ORDER BY app_id\")\n#new_train.show()\n\ntrain_p0 = new_train.filter(new_train._c1 == 0)\ntrain_p1 = new_train.filter(new_train._c1 == 1)\ntrain_p2 = new_train.filter(new_train._c1 == 2)\ntrain_p3 = new_train.filter(new_train._c1 == 3)\ntrain_p4 = new_train.filter(new_train._c1 == 4)\n#train_p0.show()\ntrain_p0 = train_p0.filter(train_p0.transaction_number > 1000)\n#train_p0.show()"}, {"cell_type": "code", "execution_count": 71, "metadata": {"id": "KO_F6jX2tlUh"}, "outputs": [], "source": "#use spark random forest\nfrom pyspark.mllib.regression import LabeledPoint\nfrom pyspark.mllib.tree import RandomForest\nfrom pyspark.ml.clustering import KMeans\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.classification import NaiveBayes\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator"}, {"cell_type": "code", "execution_count": 72, "metadata": {"id": "I8SK8j_WttXo"}, "outputs": [], "source": "#train test split using train_p0, which only include user with product 0\na, b = train_p0.randomSplit([0.8,0.2])\n\n#general classification head\nFEATURES_COL = ['app_id','maa','mia','ava','vaa','currency','operation_kind','card_type','operation_type','operation_type_group',\n                'ecommerce_flag','payment_system','income_flag','mcc','mcc_category','hour_diff','transaction_number','_c0','_c1','_c2'\n                ]\nfor col in a.columns:\n  if col in FEATURES_COL:\n    a = a.withColumn(col,a[col].cast('float'))\n#a.show()\n\nfor col in b.columns:\n  if col in FEATURES_COL:\n    b = b.withColumn(col,b[col].cast('float'))\n#b.show()\nvecAssembler = VectorAssembler(inputCols=FEATURES_COL, outputCol=\"features\")\nfrom pyspark.ml.feature import StandardScaler\nstdScaler = StandardScaler(inputCol=\"features\", \\\n                        outputCol=\"scaledFeatures\", \\\n                        withStd=True, \\\n                        withMean=False)\n\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nevaluator = MulticlassClassificationEvaluator( \\\n                  labelCol=\"_c2\", \\\n                  predictionCol=\"prediction\", \\\n                  metricName=\"accuracy\")"}, {"cell_type": "code", "execution_count": 73, "metadata": {"id": "VjLndoyMtwVv"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 952:======================>                                  (2 + 3) / 5]\r"}, {"name": "stdout", "output_type": "stream", "text": "Accuracy of LogisticRegression is = 0.969241\nTest Error of LogisticRegression = 0.0307594 \n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.ml.classification import LogisticRegression\nlr = LogisticRegression(maxIter=100, \\\n                        regParam=0.3, \\\n                        elasticNetParam=0.1, \\\n                        featuresCol=\"scaledFeatures\", \\\n                        family = \"binomial\", \\\n                        labelCol=\"_c2\")\n\nfrom pyspark.ml import Pipeline\npipeline_lr = Pipeline(stages=[vecAssembler, stdScaler, lr])\npipelineModel_lr = pipeline_lr.fit(a)\npredDF_lr = pipelineModel_lr.transform(b)\n\"\"\"\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nevaluator = MulticlassClassificationEvaluator( \\\n                  labelCol=\"_c2\", \\\n                  predictionCol=\"prediction\", \\\n                  metricName=\"accuracy\")\n\"\"\"\nlr_accuracy = evaluator.evaluate(predDF_lr)\nprint(\"Accuracy of LogisticRegression is = %g\"%(lr_accuracy))\nprint(\"Test Error of LogisticRegression = %g \"%(1.0 - lr_accuracy))"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "pPG_3Qn9t0YE"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 1030:============================================>           (4 + 1) / 5]\r"}, {"name": "stdout", "output_type": "stream", "text": "Accuracy of LogisticRegression is = 1\nTest Error of LogisticRegression = 0 \n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.ml.classification import LinearSVC\nlsvc = LinearSVC(maxIter=10, \\\n                 regParam=0.1, \\\n                 featuresCol=\"scaledFeatures\", \\\n                 labelCol=\"_c2\")\nfrom pyspark.ml import Pipeline\npipeline_lsvc = Pipeline(stages=[vecAssembler, stdScaler, lsvc])\npipelineModel_lsvc = pipeline_lsvc.fit(a)\npredDF_lsvc = pipelineModel_lsvc.transform (b) #predict test result\nlr_accuracy = evaluator.evaluate(predDF_lsvc) #measure accuracy\nprint(\"Accuracy of LogisticRegression is = %g\"%(lr_accuracy))\nprint(\"Test Error of LogisticRegression = %g \"%(1.0 - lr_accuracy))"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "GuB1Tn6ct5M7"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 1051:>                                                       (0 + 4) / 5]\r"}, {"name": "stdout", "output_type": "stream", "text": "Accuracy of Na\u00efve Bayes is = 1\nError of Na\u00efve Bayes is = 0 \n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.ml.classification import NaiveBayes\nnb = NaiveBayes(smoothing=1.0, \\\n                modelType=\"gaussian\", \\\n                featuresCol=\"scaledFeatures\", \\\n                labelCol=\"_c2\")\nfrom pyspark.ml import Pipeline\npipeline_nb = Pipeline(stages=[vecAssembler, stdScaler, nb])\npipelineModel_nb = pipeline_nb.fit(a)\npredDF_nb = pipelineModel_nb.transform (b)\nnb_accuracy = evaluator.evaluate(predDF_nb)\nprint(\"Accuracy of Na\u00efve Bayes is = %g\"%(nb_accuracy))\nprint(\"Error of Na\u00efve Bayes is = %g \"%(1.0 - nb_accuracy))"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "iioRAUmKt7nG"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 1072:>                                                       (0 + 4) / 5]\r"}, {"name": "stdout", "output_type": "stream", "text": "Accuracy of Decision Tree is = 1\nError of Decision Tree is = 0 \n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.ml.classification import DecisionTreeClassifier\n# Train a DecisionTree model.\ndt = DecisionTreeClassifier(labelCol=\"_c2\", \\\n                            featuresCol=\"scaledFeatures\", \\\n                            impurity=\"gini\")\nfrom pyspark.ml import Pipeline\npipeline_dt = Pipeline(stages=[vecAssembler, stdScaler, dt])\npipelineModel_dt = pipeline_nb.fit(a)\npredDF_dt = pipelineModel_dt.transform (b)\ndt_accuracy = evaluator.evaluate(predDF_dt)\nprint(\"Accuracy of Decision Tree is = %g\"%(dt_accuracy))\nprint(\"Error of Decision Tree is = %g \"%(1.0 - dt_accuracy))"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "mxnqLmeYt9-6"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 1121:>                                                       (0 + 4) / 5]\r"}, {"name": "stdout", "output_type": "stream", "text": "Accuracy of Random Tree is = 1\nError of Random Tree is = 0 \n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.ml.classification import RandomForestClassifier\n# Train a RandomForest model.\nrf = RandomForestClassifier(labelCol=\"_c2\", \\\n                            featuresCol=\"scaledFeatures\", \\\n                            numTrees=50)\nfrom pyspark.ml import Pipeline\npipeline_rf = Pipeline(stages=[vecAssembler, stdScaler, rf])\npipelineModel_rf = pipeline_rf.fit(a)\npredDF_rf = pipelineModel_rf.transform (b)\nrf_accuracy = evaluator.evaluate(predDF_rf)\nprint(\"Accuracy of Random Tree is = %g\"%(rf_accuracy))\nprint(\"Error of Random Tree is = %g \"%(1.0 - rf_accuracy))"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "es-8c7V3uA3j"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 1237:>                                                       (0 + 4) / 5]\r"}, {"name": "stdout", "output_type": "stream", "text": "Accuracy of Gradient-Boosted Tree is = 1\nError of Gradient-Boosted Tree is = 0 \n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.ml.classification import GBTClassifier\n# Train a GBT model.\ngbt = GBTClassifier(labelCol=\"_c2\", \\\n                    featuresCol=\"scaledFeatures\", \\\n                    maxIter=10)\nfrom pyspark.ml import Pipeline\npipeline_gbt = Pipeline(stages=[vecAssembler, stdScaler, gbt])\npipelineModel_gbt = pipeline_gbt.fit(a)\npredDF_gbt = pipelineModel_gbt.transform (b)\ngbt_accuracy = evaluator.evaluate(predDF_gbt)\nprint(\"Accuracy of Gradient-Boosted Tree is = %g\"%(gbt_accuracy))\nprint(\"Error of Gradient-Boosted Tree is = %g \"%(1.0 - gbt_accuracy))"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "kMwFK4oRuEBV"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 1544:=================================>                      (3 + 2) / 5]\r"}, {"name": "stdout", "output_type": "stream", "text": "Accuracy of One-vs-Rest is = 0.969241\nError of One-vs-Rest is = 0.0307594 \n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.ml.classification import OneVsRest\novr = OneVsRest(classifier=lr, \\\n                labelCol=\"_c2\", \\\n                featuresCol=\"scaledFeatures\")\nfrom pyspark.ml import Pipeline\npipeline_ovr = Pipeline(stages=[vecAssembler, stdScaler, ovr])\npipelineModel_ovr = pipeline_ovr.fit(a)\npredDF_ovr = pipelineModel_ovr.transform (b)\novr_accuracy = evaluator.evaluate(predDF_ovr)\nprint(\"Accuracy of One-vs-Rest is = %g\"%(ovr_accuracy))\nprint(\"Error of One-vs-Rest is = %g \"%(1.0 - ovr_accuracy))"}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "OA2pI1hhuGck"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 1762:================================>                   (126 + 4) / 200]\r"}, {"ename": "KeyboardInterrupt", "evalue": "", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)", "Input \u001b[0;32mIn [80]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n\u001b[1;32m      7\u001b[0m pipeline_fm \u001b[38;5;241m=\u001b[39m Pipeline(stages\u001b[38;5;241m=\u001b[39m[vecAssembler, stdScaler, fm])\n\u001b[0;32m----> 8\u001b[0m pipelineModel_fm \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline_fm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m predDF_fm \u001b[38;5;241m=\u001b[39m pipelineModel_fm\u001b[38;5;241m.\u001b[39mtransform (b)\n\u001b[1;32m     10\u001b[0m fm_accuracy \u001b[38;5;241m=\u001b[39m evaluator\u001b[38;5;241m.\u001b[39mevaluate(predDF_fm)\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/base.py:161\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/pipeline.py:114\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    112\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mtransform(dataset)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m indexOfLastEstimator:\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/base.py:161\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/wrapper.py:335\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset):\n\u001b[0;32m--> 335\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/wrapper.py:332\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;124;03mFits a Java model to the input dataset.\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;124;03m    fitted Java model\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n", "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1303\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1296\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1305\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n", "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1033\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1033\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1035\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n", "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1200\u001b[0m, in \u001b[0;36mGatewayConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JNetworkError(\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while sending\u001b[39m\u001b[38;5;124m\"\u001b[39m, e, proto\u001b[38;5;241m.\u001b[39mERROR_ON_SEND)\n\u001b[1;32m   1199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1200\u001b[0m     answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m   1201\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m   1202\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m answer\u001b[38;5;241m.\u001b[39mstartswith(proto\u001b[38;5;241m.\u001b[39mRETURN_MESSAGE):\n", "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n", "\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}], "source": "from pyspark.ml.classification import FMClassifier\n# Train a FM model.\nfm = FMClassifier(labelCol=\"_c2\", \\\n                  featuresCol=\"scaledFeatures\", \\\n                  stepSize=0.001)\nfrom pyspark.ml import Pipeline\npipeline_fm = Pipeline(stages=[vecAssembler, stdScaler, fm])\npipelineModel_fm = pipeline_fm.fit(a)\npredDF_fm = pipelineModel_fm.transform (b)\nfm_accuracy = evaluator.evaluate(predDF_fm)\nprint(\"Accuracy of Factorization Machines is = %g\"%(fm_accuracy))\nprint(\"Error of Factorization Machines is = %g \"%(1.0 - fm_accuracy))"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}], "metadata": {"colab": {"name": "TheEnd.ipynb", "provenance": []}, "kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.13"}}, "nbformat": 4, "nbformat_minor": 4}